{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Auto-Correct\n",
    "\n",
    "Welcome to the first assignment of Course 2. This assignment is here to help you brush up on your python and probability skills. In doing so, you will implement an auto-correct system that is very effective and useful. By the end, you will be surprised by the accuracy of your model. \n",
    "You use autocorrect everyday in your cell phone and computer. In this assignment, you will explore what really goes on behind the scenes. Of course, the model you are about to implement is not identical to the one used in your phone, but it is good enough to surprise you. By completing this assignment you will learn how to: \n",
    "\n",
    "- Get a word count given a corpus\n",
    "- Get a word probability in the corpus \n",
    "- Manipulate strings \n",
    "- Filter strings \n",
    "- Implement Minimum edit distance to compare strings and to help find the optimal path for the edits. \n",
    "- Understand how dynamic programming works\n",
    "\n",
    "\n",
    "Similar systems are used everywhere. For example, if you type in the word **\"I am lerningg\"**, chances are very high that you meant to write **\"learning\"**, as shown in **Figure 1**. In this assignment, you will learn how to come up with that specific probability and in the next assignment you will learn how predict the word **\"from\"** given that **\"learning\"** was already written. This will help lay the foundations for more sophisticated NLP tasks, and would allow you to understand natural language processing better when implementing systems like machine translation and speech recognition in Course 4 and 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='auto-correct.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:250px;\" /> Figure 1 </div>\n",
    "\n",
    "In this assignment, you will implement models that correct words that are 1 and 2 edit distances away. \n",
    "We say two words are n edit distance away from each other when we need n edits to change one word into another. An edit could consist of one of the following options: \n",
    "\n",
    "    - add a letter: ‘te’ => ‘the, ten, ate, ...’\n",
    "    - remove a letter: ‘hat’ => ‘at, ha, ht’\n",
    "    - substitute a letter: ‘jat’ => ‘hat, rat, cat, mat, ...’\n",
    "    - switch two adjacent letters: ‘eta’ => ‘eat, tea,...’\n",
    "\n",
    "You will be using the four methods above to implement an Auto-correct. To do so, you will need to compute probabilities of a certain word being correct given an input. This auto-correct you are about to implement was first created by [Peter Norvig](https://en.wikipedia.org/wiki/Peter_Norvig) in 2007. The goal of the model is to compute the following probability:\n",
    "\n",
    "$$P(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{1}$$\n",
    "\n",
    "The equation above is essentially just bayes rule. Equation 1 says that the probability of a word being correct is equal to the probability of having a certain word given that it is correct time the probability of being correct divided by the probability of a word appearing. To compute equation one we will first import a data set and then create all the probabilities we need using that data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any other machine learning task, the first thing you have to do is process your data set. Many courses load in the data for you. However, in the real world you will be all alone creating these NLP systems. Therefore, we feel like it is important for you to load in the dataset and process it alone. Your first task is to read in a file called 'shakespeare.txt' which is found on your Coursera hub. To look at this file you can go to File ==> Open. \n",
    "\n",
    "**Instructions**: Implement the function `process_data` which reads in a corpus, changes everything to lower case, and returns a list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: process_data\n",
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in a corpus in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    data = open(file_name).read()\n",
    "    words = re.findall(r'\\w+', data.lower())\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nature\n"
     ]
    }
   ],
   "source": [
    "words = process_data('shakespeare.txt')\n",
    "vocab = set(words)  # this will be your new vocabulary and remove the duplicate\n",
    "# print (words.index('nature'))\n",
    "# indexes = [i for i,j in enumerate(words) if j == 'nature']\n",
    "# print (indexes)\n",
    "print(words[2567])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Expected Output:*** nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a list of words that you just imported. You will start by implementing a get_count function that returns a dictionary mapping from the word to its counts in the corpus. For example, given the following sentence: **\"I am happy because I am learning\"**, your dictionary should return the following: \n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b>Key </b>  </td>\n",
    "    <td> <b>Value </b> </td> \n",
    "\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> I  </td>\n",
    "    <td> 2</td> \n",
    " \n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td>am</td>\n",
    "    <td>2</td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td>happy</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>because</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td>learning</td>\n",
    "    <td>1</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Instructions**: \n",
    "Implement a `get_count` which returns the a dictionary where the key is a word and the value is its frequency in a list of words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: word_count\n",
    "def get_count(words):\n",
    "    '''\n",
    "    Input:\n",
    "        words: a list of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The key is the word and value is the frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # return this variable correctly\n",
    "    ### START CODE HERE \n",
    "    word_count_dict = Counter(words)\n",
    "    ### END CODE HERE ### \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6116\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(words)\n",
    "print(len(word_count_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (word_count_dict['muse'])\n",
    "# print (word_count_dict.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "6116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function you will implement will take in the dictionary and compute the probability of each word happening. To compute the probability of a certain word happening, you have to calculate:\n",
    "$$P(w_i) = \\frac{C(w_i)}{|V|} \\tag{2}$$\n",
    "Where $|V|$ is the cardinality, or unique number of words in your corpus. $C(w_i)$ is the total number of times $w_i$ appears in the corpus. For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'**:\n",
    "\n",
    "$$P(am) = \\frac{C(w_i)}{|V|} = \\frac {2}{7} \\tag{3}.$$\n",
    "\n",
    "**Instructions:** Implement `get_probs` function which gives you the probability \n",
    "of a word happening. This returns a dictionary mapping from the word to its probability in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_probs\n",
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: a dictionary where the keys are the words and their frequencies are the values\n",
    "    Output:\n",
    "        probs: a dictionary where the keys are the words and the values are the probability of each word occuring. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    n = sum(word_count_dict.values())\n",
    "    for w in word_count_dict.keys():\n",
    "        probs[w] = word_count_dict[w] / n\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028444063117842356\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(word_count_dict)\n",
    "print(probs['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: 0.028444063117842356. You can see that the word 'the' has around a 5% chance of happening which is very logical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: String Manipulations\n",
    "\n",
    "Now, that you have computed $P(w_i)$ for all the words in the corpus, You will write a few functions to manipulate strings so that you can edit the errors and return the right spelling mistake. In this section, you will implement four functions: \n",
    "\n",
    "* deletes: given a word, it returns all the possible words that are in the vocabulary that have one missing character. \n",
    "* switches: given a word, it returns all the possible words that are in the vocabulary that have two adjacent letters  switched.\n",
    "* inserts: given a word, it returns all the possible words that are in the vocabulary that have an additional character inserted in some index.\n",
    "* replaces: given a word, it returns all the possible words that are in the vocabulary that have one character being replaced in some index.\n",
    "\n",
    "\n",
    "**Instructions:** Implement a `deletes` function that given a certain word, returns a set of strings with one possible delete. For example, given the word 'nice' it would return the set: {'ice', 'nce', 'nic', 'nie'}. Make sure you only return the words that are in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice\n",
      "\n",
      "\n",
      "nice\n"
     ]
    }
   ],
   "source": [
    "# This is a sample test\n",
    "word_test = 'nice'\n",
    "L_test = word_test[:4]    #index 0-1\n",
    "R_test = word_test[4:]    #index 2:length(word_test - 1)\n",
    "print (L_test)\n",
    "print (R_test)\n",
    "print (R_test[1:])\n",
    "print (L_test + R_test[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: deletes\n",
    "def deletes(word, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "        word: string\n",
    "    Output:\n",
    "        deletes: a set of all possible, make sure the possible words in vocabulary \n",
    "    '''\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]    # This is to remove index i, NOTE: 'If' condition can also be len(R) > 1 See above exmpale.\n",
    "    deletes = set(deletes).intersection(set(vocab))    # Frank comment: vocab is already a set, no need to set it again. \n",
    "    return set(deletes)    # Frank comment: make it as set again???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ice'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing your implementation\n",
    "deletes('nice', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Now implement a function that switches two letters in a word. It takes in a word and returns all the possible switches that are adjacent. For example, given the word eta, it returns {'eat', 'tea'}. Make sure you only return the words that are in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: switches\n",
    "def switches(word, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "        word: string\n",
    "    Output:\n",
    "        switches: a set of all possible words with one switches \n",
    "    ''' \n",
    "    switches = set()   # set this correctly\n",
    "    ### START CODE HERE ###\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    switches = [L + R[1] + R[0] + R[2:] for L, R in splits if len (R)>1]\n",
    "    switches = set(switches).intersection(set(vocab))\n",
    "    ### END CODE HERE ###\n",
    "    return switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eat'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing your implementation\n",
    "switches('eta', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Now you should implement a function that takes in a word and returns a set of words with one replaced letter from the original word. Make sure that you only return the words that are in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: replaces\n",
    "def replaces(word, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "        word: string\n",
    "    Output:\n",
    "        replaces: a set of all possible words where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    replaces= set() # set this correctly\n",
    "    ### START CODE HERE ###\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    replaces = set(replaces).intersection(set(vocab))\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return set(replaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nice', 'nile', 'nine', 'vice'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaces('nice',vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: {'nice', 'nile', 'nine', 'vice'}\n",
    "\n",
    "**Instructions**: Now you will implement a function that takes in a word and inserts a letter at every index. It then returns the new set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: inserts\n",
    "def inserts(word, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "        word: string\n",
    "    Output:\n",
    "        inserts: a set of all possible words with one new letter inserted at every index\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    ### START CODE HERE ###\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    inserts = set(inserts).intersection(set(vocab))\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return set(inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'act', 'alt', 'art', 'cat', 'eat', 'sat'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inserts('at', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Combining the edits\n",
    "\n",
    "Now that you have implemented the string manipulations, you will create two functions that given a word would return all the possible single edits on that word. You will then use that function to generate two edits. You can keep using it for as many edits as you want, but for this assignment you will only perform two edits on an error. \n",
    "\n",
    "**Instructions**: Implement the `one_edit` function to get all the possible edits that are one edit away from a word. The edits could consist of either a replace, insert, delete, or switch operation. You should use the previous functions you have already implemented to complete this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_edit\n",
    "def one_edit(word, vocab):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: a string\n",
    "        vocab: a set of vocabulary in your corpus\n",
    "    Output:\n",
    "        one_edits: a set of words with one possible edit, all the words are in the vocabulary.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    one_edits = inserts(word, vocab) | replaces(word,vocab) | switches(word, vocab) | deletes(word, vocab)\n",
    "#     one_edits = inserts(words, vocab).union(replaces(word,vocab))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ours', 'hours', 'yours', 'your'}\n"
     ]
    }
   ],
   "source": [
    "s = one_edit(\"yours\", vocab)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can generalize this to implement to get two edits on a word. To do so, you would have to get all the possibble edits on a single word and then for each modified word, you would have to modify it again. \n",
    "\n",
    "**Instructions**: Implement the two_edits function that returns a list of all words that are two edits away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_edits\n",
    "def two_edits(word, vocab):\n",
    "    '''\n",
    "    Input:\n",
    "        word: a string\n",
    "        vocab: a set of vocabulary in your corpus\n",
    "    Output:\n",
    "        one_edits: a set of words with one possible edit, all the words are in the vocabulary.\n",
    "    '''\n",
    "    final_words = set()\n",
    "    edits = one_edit(word,vocab)\n",
    "    final_words = final_words | edits\n",
    "    for cur_word in edits:\n",
    "        new_set = one_edit(cur_word,vocab)\n",
    "        final_words = final_words | new_set    # update the final words set\n",
    "    return final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mother', 'mothers', 'other', 'others', 'smother'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if your implementation makes sense\n",
    "two_edits(\"mother\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: {'sneaking', 'speaking', 'stealing'}\n",
    "\n",
    "Now you will use your `two_edits` function to get a set of all the possible 2 edits on your word. You will then use those words to get the most probable word you meant to type.\n",
    "**Instructions**: Implement `get_corrections`, which returns the word if it is found in the vocabulary and a dictionary of n possible suggestions (where the key is the proposed word and the value is its probability) otherwise. You should use your two_edits functions. You can also use `most_common` which is a helper function inside Counter. Good luck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_test = {\"abc\":0.1, \"bcd\": 0.4, \"cde\":0.2}\n",
    "# dict_count = Counter(dict_test)\n",
    "# dict_count.most_common(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: get_corrections\n",
    "def get_corrections(word, probs, vocab, n=2):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a string \n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        best_words: a dictionary with the most probable n corrected words. The values are the proabilities.\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if word in vocab:\n",
    "        return word\n",
    "    s = two_edits(word, vocab)\n",
    "    best_words = {}\n",
    "    for w in s:\n",
    "        curr_probability = probs[w]\n",
    "        best_words[w] = curr_probability\n",
    "    best_words = Counter(best_words)    # key: word; value: frequency\n",
    "    best_words = best_words.most_common(n)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wars', 0.00031708135934643936), ('mars', 0.00022382213600925132)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'mahs' \n",
    "get_corrections(my_word,probs, vocab, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Minimum Edit distance\n",
    "\n",
    "Now that you have implemented your auto-correct, How do you evaluate the similarity between two strings for example: 'waht' and 'what'? Also how do you efficiently find the shortest path to go from the word, 'waht' to the word 'what'? You will implement a dynamic programming system that will not only tell you the minimum amount of edits required to convert a string into another string, but also tells you in what order to convert these strings. In lecture, you saw minimum edit distance. You have to create a matrix and update each element in the matrix as follows:\n",
    "\n",
    "$$ D[i,j]=\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del-cost(source[i])\\\\\n",
    "D[i-1,j-1] + sub\\_cost(source[i],target[j])\\\\\n",
    "D[i,j-1] + ins-cost(target[j])\n",
    "\\end{cases} \\tag{4}$$\n",
    "So converting the word **play** to **stay**, using an input cost of one, a delete cost of 1, and replace cost of 2 would give you the following table:\n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> </b>  </td>\n",
    "    <td> <b># </b>  </td>\n",
    "    <td> <b>s </b>  </td>\n",
    "    <td> <b>t </b> </td> \n",
    "    <td> <b>a </b> </td> \n",
    "    <td> <b>y </b> </td> \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td> <b>  #  </b></td>\n",
    "    <td> 0</td> \n",
    "    <td> 1</td> \n",
    "    <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    " \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>  p  </b></td>\n",
    "    <td> 1</td> \n",
    " <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    "   <td> 5</td>\n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td> <b> l </b></td>\n",
    "    <td>2</td> \n",
    "    <td>3</td> \n",
    "    <td>4</td> \n",
    "    <td>5</td> \n",
    "    <td>6</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> a </b></td>\n",
    "    <td>3</td> \n",
    "     <td>4</td> \n",
    "     <td>5</td> \n",
    "     <td>4</td>\n",
    "     <td>5</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td> <b> y </b></td>\n",
    "    <td>4</td> \n",
    "      <td>5</td> \n",
    "     <td>6</td> \n",
    "     <td>5</td>\n",
    "     <td>4</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "**Instructions**: Implement the function below to get the minimum amount of edits required given a source string and a target string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: min_edit_distance\n",
    "def min_edit_distance(source, target):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the word you are starting with\n",
    "        target: a string corresponding to the word you want to end with\n",
    "    Output:\n",
    "        D: a matrix of len(source) by len(target)\n",
    "        D[m,n]: the minimum number of edits required to convert the source string to the target\n",
    "        \n",
    "    '''\n",
    "    # use del_cost = 1, ins_cost = 1, and rep_cost = 2\n",
    "    del_cost = 1\n",
    "    ins_cost = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "    D = np.zeros((m+1, n+1), dtype=int) # row 0 and column 0 is the distance from the empty string\n",
    "    ### START CODE HERE ###\n",
    "    for row in range(1,m+1):\n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "    for col in range(1,n+1):\n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "    for row in range(1,m+1):\n",
    "        for col in range(1,n+1):\n",
    "            rep_cost = 2\n",
    "            if source[row-1] == target[col-1]:\n",
    "                rep_cost = 0\n",
    "            # delete, insert, replace\n",
    "            D[row,col] = min(D[row-1,col] + 1, D[row,col-1]+1, D[row-1,col-1] + rep_cost) # get the matrix\n",
    "    ### END CODE HERE ###\n",
    "    return D, D[m, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n",
      "s  5  4  5  6  5\n"
     ]
    }
   ],
   "source": [
    "# testing your implementation\n",
    "source =  'plays'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "idx = list(source)\n",
    "idx.insert(0, '#')\n",
    "cols = list(target)\n",
    "cols.insert(0, '#')\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Optional - Backtrace\n",
    "\n",
    "Once you have computed your matrix using minimum edit distance, how would find the shortest path from the top left corner to the bottom right corner? Note you could use backtrace. Make sure you submit your assignment before you modify anything. Try to find the shortest path given the matrix that your `min_edit_distance` function returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with back trace - insert your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References\n",
    "- Dan Jurafsky - Speech and Language Processing - Textbook\n",
    "- This auto-correct explanation was first done by Peter Norvig in 2007 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "ml-test-jupyter",
   "graded_item_id": "qXyCq",
   "launcher_item_id": "QD5Yp"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
